{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "source": [
        "# Learning Graph Embeddings with HMLET (End) Model\n",
        "\n",
        "Kong et. al. in their paper *Linear, or Non-Linear, That is the Question!* discussed about the issue of manually deciding whether to go with linear transformation or non-linear transformation while learning the graph embeddings. To address this issue, they proposed a method that will automate this manual decision. In other words, for each node of the graph (and at each later), model will learn an additional binary vector of length 2 (i.e. 1Ô∏è‚É£0Ô∏è‚É£ would indicate linear-transformation of the given node, similarly, 0Ô∏è‚É£1Ô∏è‚É£ means a non-linear transformation).\n",
        "\n",
        "The authors dubbed the model **H**ybrid **M**ethod of **L**inear and nonlin**E**ar collaborative fil**T**ering (HMLET, pronounced as Hamlet).\n",
        "\n",
        "There are four variants:\n",
        "\n",
        "![Four variants of HMLET in terms of the location of the non-linear propagation. HMLET(End) shows the best accuracy in experiments. It was known that the problem of over-smoothing happens with more than 2 non-linear propagation layers, and therefore we are using up to 2 non-linear layers.](https://github.com/sparsh-ai/stanza/raw/S693545/images/img0.png)\n",
        "\n",
        "Four variants of HMLET in terms of the location of the non-linear propagation. HMLET(End) shows the best accuracy in experiments. It was known that the problem of over-smoothing happens with more than 2 non-linear propagation layers, and therefore we are using up to 2 non-linear layers.\n",
        "\n",
        "Each method except HMLET(All) uses up to 2 non-linear layers since it is known that more than 2 non-linear layers cause the problem of over-smoothing. Authors tested various options of where to put them. First, HMLET(Front) focuses on the fact that GCNs are highly influenced by close neighborhood, i.e., in the first and second layers. Therefore, HMLET(Front) adopts the gating module in the front and uses only the linear propagation layers afterwards. Second, HMLET(Middle) only uses the linear propagation in the front and last and then adopts the gating module in the second and third layers. Last, as the gating module is located in the third and fourth layers, HMLET(End) focuses on gating in the third and fourth layers.\n",
        "\n",
        "![The comparison of NDCG@20 with all types of HMLET in three public benchmarks.](https://github.com/sparsh-ai/stanza/raw/S693545/images/img1.png)\n",
        "\n",
        "The comparison of NDCG@20 with all types of HMLET in three public benchmarks.\n",
        "\n",
        "Experiments revealed that the HMLET (End) variant gave superior performance on the above three public datasets. So we will focus on this variant for our in-depth analysis.\n",
        "\n",
        "**HMLET (End) Model**\n",
        "\n",
        "Here is the architecture diagram of HMLET (End) and the algorithm to train this model:\n",
        "\n",
        "![The detailed workflow of HMLET(End). This model variant bypasses the nonlinearity propagation on the first and second layers to address the over-smoothing problem and then propagates the non-linear embedding in the third and fourth layers. We prepare both the linear and non-linear propagation steps in a layer and let our gating module with STGS (straight-through Gumbel softmax) decide which one to use for each node. For instance, it can select a sequence of linear ‚Üí linear ‚Üí linear ‚Üí non-linear for some nodes while it can select a totally different sequence for other nodes.](https://github.com/sparsh-ai/stanza/raw/S693545/images/img2.png)\n",
        "\n",
        "The detailed workflow of HMLET(End). This model variant bypasses the nonlinearity propagation on the first and second layers to address the over-smoothing problem and then propagates the non-linear embedding in the third and fourth layers. We prepare both the linear and non-linear propagation steps in a layer and let our gating module with STGS (straight-through Gumbel softmax) decide which one to use for each node. For instance, it can select a sequence of linear ‚Üí linear ‚Üí linear ‚Üí non-linear for some nodes while it can select a totally different sequence for other nodes.\n",
        "\n",
        "![Untitled](https://github.com/sparsh-ai/stanza/raw/S693545/images/img3.png)\n",
        "\n",
        "**Linear embedding propagation update equation (equation #6):**\n",
        "\n",
        "![Untitled](https://github.com/sparsh-ai/stanza/raw/S693545/images/img4.png)\n",
        "\n",
        "Here, $\\dfrac{1}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_v|}}$ is a symmetric normalization term to restrict the scale of embeddings into a reasonable boundary.\n",
        "\n",
        "**Non-linear embedding propagation update equation (equation #7):**\n",
        "\n",
        "![Untitled](https://github.com/sparsh-ai/stanza/raw/S693545/images/img5.png)\n",
        "\n",
        "where ùúô is a non-linear activation, e.g., ELU, Leaky ReLU, etc.\n",
        "\n",
        "**BPR Loss (equation #9):**\n",
        "\n",
        "![Untitled](https://github.com/sparsh-ai/stanza/raw/S693545/images/img6.png)\n",
        "\n",
        "where ùúé is the sigmoid function. $Œò$ is the initial embeddings and the parameters of the gating modules, and ùúÜ controls the $ùêø_2$ regularization strength. We use each observed user-item interaction as a positive instance and a bunch of negative instances selected using negative-sampling strategy.\n",
        "\n",
        "Here is the PyTorch implementation of this HMLET (End) model (note that for clarity, only showing the core functions):\n",
        "\n",
        "```python\n",
        "class HMLET_End(nn.Module):\n",
        "\n",
        "\tdef __choosing_one(self, features, gumbel_out):\n",
        "\t\tfeature = torch.sum(torch.mul(features, gumbel_out), dim=1)  # batch x embedding_dim (or batch x embedding_dim x layer_num)\n",
        "\t\treturn feature\n",
        "\n",
        "\tdef computer(self, gum_temp, hard):     \n",
        "\t\t\n",
        "\t\tself.Graph = self.g_train   \n",
        "\t\tif self.dropout:\n",
        "\t\t\tif self.training:\n",
        "\t\t\t\tg_droped = self.__dropout(self.keep_prob)\n",
        "\t\t\telse:\n",
        "\t\t\t\tg_droped = self.Graph        \n",
        "\t\telse:\n",
        "\t\t\tg_droped = self.Graph\n",
        "    \n",
        "\t\t# Init users & items embeddings  \n",
        "\t\tusers_emb = self.embedding_user.weight\n",
        "\t\titems_emb = self.embedding_item.weight\n",
        "      \n",
        "\t\t## Layer 0\n",
        "\t\tall_emb_0 = torch.cat([users_emb, items_emb])\n",
        "\t\t\n",
        "\t\t# Residual embeddings\n",
        "\t\tembs = [all_emb_0]\n",
        "\t\n",
        "\t\t## Layer 1\n",
        "\t\tall_emb_lin_1 = torch.sparse.mm(g_droped, all_emb_0)\n",
        "\t\t\n",
        "\t\t# Residual embeddings\t\n",
        "\t\tembs.append(all_emb_lin_1)\n",
        "   \n",
        "\t\t## layer 2\n",
        "\t\tall_emb_lin_2 = torch.sparse.mm(g_droped, all_emb_lin_1)\n",
        "\t\t\n",
        "\t\t# Residual embeddings\n",
        "\t\tembs.append(all_emb_lin_2)\n",
        "\t\t\n",
        "\t\t## layer 3\n",
        "\t\tall_emb_lin_3 = torch.sparse.mm(g_droped, all_emb_lin_2)\n",
        "\t\tall_emb_non_1 = self.activation_function(torch.sparse.mm(g_droped, all_emb_0))\n",
        "\t\t\n",
        "\t\t# Gating\n",
        "\t\tstack_embedding_1 = torch.stack([all_emb_lin_3, all_emb_non_1],dim=1)\n",
        "\t\tconcat_embeddings_1 = torch.cat((all_emb_lin_3, all_emb_non_1),-1)\n",
        "\n",
        "\t\tgumbel_out_1, lin_count_3, non_count_3 = self.gating_network_list[0](concat_embeddings_1, gum_temp, hard, self.config['division_noise'])\n",
        "\t\tembedding_1 = self.__choosing_one(stack_embedding_1, gumbel_out_1)\n",
        "\n",
        "\t\t# Residual embeddings\n",
        "\t\tembs.append(embedding_1)\n",
        "  \t\n",
        "\t\t# layer 4\n",
        "\t\tall_emb_lin_4 = torch.sparse.mm(g_droped, embedding_1)\n",
        "\t\tall_emb_non_2 = self.activation_function(torch.sparse.mm(g_droped, embedding_1))\n",
        "\t\t\n",
        "\t\t# Gating\n",
        "\t\tstack_embedding_2 = torch.stack([all_emb_lin_4, all_emb_non_2],dim=1)\n",
        "\t\tconcat_embeddings_2 = torch.cat((all_emb_lin_4, all_emb_non_2),-1)\n",
        "\n",
        "\t\tgumbel_out_2, lin_count_4, non_count_4 = self.gating_network_list[1](concat_embeddings_2, gum_temp, hard, self.config['division_noise'])\n",
        "\t\tembedding_2 = self.__choosing_one(stack_embedding_2, gumbel_out_2)\n",
        "\n",
        "\t\t# Residual embeddings  \t\t\n",
        "\t\tembs.append(embedding_2)\n",
        "\n",
        "\t\t## Stack & mean residual embeddings\n",
        "\t\tembs = torch.stack(embs, dim=1)\n",
        "\t\tlight_out = torch.mean(embs, dim=1)\n",
        "   \n",
        "\t\tusers, items = torch.split(light_out, [self.num_users, self.num_items])\n",
        "\t\t\n",
        "\t\treturn users, items, [lin_count_3, non_count_3, lin_count_4, non_count_4], embs\n",
        "\n",
        "\tdef getUsersRating(self, users, gum_temp, hard):\n",
        "\t\tall_users, all_items, gating_dist, embs = self.computer(gum_temp, hard)\n",
        "\t\t\n",
        "\t\tusers_emb = all_users[users.long()]\n",
        "\t\titems_emb = all_items\n",
        "\n",
        "\t\trating = self.activation_function(torch.matmul(users_emb, items_emb.t()))\n",
        "\n",
        "\t\treturn rating, gating_dist, embs\n",
        "\n",
        "\tdef getEmbedding(self, users, pos_items, neg_items, gum_temp, hard):\n",
        "\t\tall_users, all_items, gating_dist, embs = self.computer(gum_temp, hard)\n",
        "\t\t\n",
        "\t\tusers_emb = all_users[users]\n",
        "\t\tpos_emb = all_items[pos_items]\n",
        "\t\tneg_emb = all_items[neg_items]\n",
        "\n",
        "\t\tusers_emb_ego = self.embedding_user(users)\n",
        "\t\tpos_emb_ego = self.embedding_item(pos_items)\n",
        "\t\tneg_emb_ego = self.embedding_item(neg_items)\n",
        "\n",
        "\t\treturn users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego, gating_dist, embs\n",
        "\n",
        "\tdef bpr_loss(self, users, pos, neg, gum_temp, hard):\n",
        "\t\t(users_emb, pos_emb, neg_emb, \n",
        "\t\tuserEmb0,  posEmb0, negEmb0, gating_dist, embs) = self.getEmbedding(users.long(), pos.long(), neg.long(), gum_temp, hard)\n",
        "\t\t\n",
        "\t\treg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
        "\t\t\t\t\t\t\tposEmb0.norm(2).pow(2)  +\n",
        "\t\t\t\t\t\t\tnegEmb0.norm(2).pow(2))/float(len(users))\n",
        "\t\t\n",
        "\t\tpos_scores = torch.mul(users_emb, pos_emb)\n",
        "\t\tpos_scores = torch.sum(pos_scores, dim=1)\n",
        "\t\tneg_scores = torch.mul(users_emb, neg_emb)\n",
        "\t\tneg_scores = torch.sum(neg_scores, dim=1)\n",
        "\t\t\n",
        "\t\tloss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
        "\t\t\n",
        "\t\treturn loss, reg_loss, gating_dist, embs\n",
        "\t\t\n",
        "\tdef forward(self, users, items, gum_temp, hard):\n",
        "\t\t# compute embedding\n",
        "\t\tall_users, all_items, gating_dist, embs = self.computer(gum_temp, hard)\n",
        "\n",
        "\t\tusers_emb = all_users[users]\n",
        "\t\titems_emb = all_items[items]\n",
        "\n",
        "\t\tinner_pro = torch.mul(users_emb, items_emb)\n",
        "\t\tgamma     = torch.sum(inner_pro, dim=1)\n",
        "\n",
        "\t\treturn gamma, gating_dist, embs\n",
        "```\n",
        "\n",
        "**Gating Module**\n",
        "\n",
        "Gating module is the core component of the model. Here is the algorithm to calculate the gating decision vectors (i.e. 1Ô∏è‚É£0Ô∏è‚É£ would indicate linear-transformation of the given node, similarly, 0Ô∏è‚É£1Ô∏è‚É£ means a non-linear transformation):\n",
        "\n",
        "![The intuition behind this technique is that i) the embeddings of nodes may exhibit both the linearity and the non-linearity in their characteristics and ii) the linearity and the non-linearity of nodes may vary from one layer to another.](https://github.com/sparsh-ai/stanza/raw/S693545/images/img7.png)\n",
        "\n",
        "The intuition behind this technique is that i) the embeddings of nodes may exhibit both the linearity and the non-linearity in their characteristics and ii) the linearity and the non-linearity of nodes may vary from one layer to another.\n",
        "\n",
        "If ùúâ is the first or second type, a designated embedding type is selected. If ùúâ is the third type, the input embeddings, i.e., the linear and non-linear embedding, are concatenated and then passed to an MLP (multi-layer perceptron) (Lines 7 and 8 in Algorithm 1). The result of the MLP is a logit vector ùíç, an input for STGS (Line 9). The logit vector ùíç corresponds to log ùùÖ in gumbel-softmax formulation. ùíà represents a linear or non-linear selection by the gating module, i.e., ùíà is a two-dimensional one-hot vector. Therefore, ùíÜ ùê∫ is the same as either of ùíÜ ùêø or ùíÜ ùëÅ (Line 10).\n",
        "\n",
        "[Here](https://colab.research.google.com/gist/sparsh-ai/a46167213b2b308953b2654381c8725b/t611269-learning-graph-embeddings-of-gowalla-dataset-using-hmlet-model.ipynb#scrollTo=XvBM9e8sx8Wl&line=38&uniqifier=1) is the PyTorch implementation of this gating module:\n",
        "\n",
        "```python\n",
        "class Gating_Net(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, mlp_dims):\n",
        "        super(Gating_Net, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.softmax =  nn.LogSoftmax(dim=1)\n",
        "        fc_layers = []\n",
        "        for i in range(len(mlp_dims)):\n",
        "            if i == 0:\n",
        "                fc_layers.append(nn.Linear(embedding_dim*2, mlp_dims[i]))\n",
        "            else:\n",
        "                fc_layers.append(nn.Linear(mlp_dims[i-1], mlp_dims[i]))\t\n",
        "            if i != len(mlp_dims) - 1:\n",
        "                fc_layers.append(nn.BatchNorm1d(mlp_dims[i]))\n",
        "                fc_layers.append(nn.ReLU(inplace=True))\n",
        "        self.mlp = nn.Sequential(*fc_layers)\n",
        "\n",
        "    def gumbel_softmax(self, logits, temperature, division_noise, hard):\n",
        "        \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
        "        Args:\n",
        "          logits: [batch_size, n_class] unnormalized log-probs\n",
        "          temperature: non-negative scalar\n",
        "          hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
        "        Returns:\n",
        "          [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
        "          If hard=True, then the returned sample will be one-hot, otherwise it will\n",
        "          be a probabilitiy distribution that sums to 1 across classes\n",
        "        \"\"\"\n",
        "        y = self.gumbel_softmax_sample(logits, temperature, division_noise) ## (0.6, 0.2, 0.1,..., 0.11)\n",
        "        if hard:\n",
        "            k = logits.size(1) # k is numb of classes\n",
        "            # y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)  ## (1, 0, 0, ..., 0)\n",
        "            y_hard = torch.eq(y, torch.max(y, dim=1, keepdim=True)[0]).type_as(y)\n",
        "            y = (y_hard - y).detach() + y\n",
        "        return y\n",
        "\n",
        "    def gumbel_softmax_sample(self, logits, temperature, division_noise):\n",
        "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
        "        noise = self.sample_gumbel(logits)\n",
        "        y = (logits + (noise/division_noise)) / temperature\n",
        "        return F.softmax(y)\n",
        "\n",
        "    def sample_gumbel(self, logits):\n",
        "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
        "        noise = torch.rand(logits.size())\n",
        "        eps = 1e-20\n",
        "        noise.add_(eps).log_().neg_()\n",
        "        noise.add_(eps).log_().neg_()\n",
        "        return Variable(noise.float()).cuda()\n",
        "\n",
        "    def forward(self, feature, temperature, hard, division_noise): #z= batch x z_dim // #feature =  batch x num_gen x 256*8*8\n",
        "        x = self.mlp(feature)\n",
        "        out = self.gumbel_softmax(x, temperature, division_noise, hard)\n",
        "        out_value = out.unsqueeze(2)\n",
        "        out = out_value.repeat(1, 1, self.embedding_dim)\n",
        "                \n",
        "        return out, torch.sum(out_value[:,0]), torch.sum(out_value[:,1])\n",
        "```\n",
        "\n",
        "**Experiment on Gowalla dataset**\n",
        "\n",
        "We filtered out those users and items with less than ten interactions in all datasets, i.e., a 10-core setting. For testing, we then split a dataset into training (80%), validation (10%), and test (10%) sets. We choose the best hyperparameter set via the grid search with the validation set. The best setting found in HMLET is as follows: the number of linear layers is set to 4; the number of non-linear layers is set to 2 for HMLET(End); the optimizer is Adam; the learning rate is 0.001; the $ùêø_2$ regularization coefficient ùúÜ is 1E-4; the mini-batch size is 2,048; the dropout rate is 0.4. And we use the temperature ùúè with an initialization to 0.7, a minimum temperature of 0.01, and a decay factor of 0.995. Also, for fair comparison, we set the embedding sizes for all methods to 512. In non-linear layers, we test two non-linear activation functions: Leaky-ReLU (negative slope = 0.01) and ELU (ùõº = 1.0).\n",
        "\n",
        "Here are the parameters:\n",
        "\n",
        "```jsx\n",
        "==============================\n",
        "Model: HMLET_End\n",
        "Model config: {'embedding_dim': 512, 'activation_function': 'elu', 'dropout': 1, 'keep_prob': 0.6, 'a_split': 0, 'gating_mlp_dims': [128, 2]}\n",
        "Dataset: gowalla\n",
        "EPOCHS: 4\n",
        "Pretrain: False\n",
        "BPR batch size: 2048\n",
        "Test batch size: 100\n",
        "Test topks: [10, 20, 30, 40, 50]\n",
        "N fold: 100\n",
        "Tensorboard: True\n",
        "==============================\n",
        "==============================\n",
        "DATA PATH: /content/data/gowalla\n",
        "SAVE FILE PATH: /content/checkpoints/HMLET_End/gowalla\n",
        "LOAD FILE PATH: /content/checkpoints/HMLET_End/gowalla/\n",
        "EXCEL PATH: /content/excel\n",
        "BOARD PATH: /content/tensorboard\n",
        "==============================\n",
        "==============================\n",
        "Cuda: True\n",
        "CUDA device: 0\n",
        "==============================\n",
        "==============================\n",
        "Multicore: 1\n",
        "CORES: 1\n",
        "==============================\n",
        "```\n",
        "\n",
        "We ran it for 4 epochs. Here are the results:\n",
        "\n",
        "```jsx\n",
        "Loading /content/data/gowalla\n",
        "==============================\n",
        "810128 interactions for training\n",
        "108621 interactions for testing\n",
        "<__main__.Loader object at 0x7fb5d94e2510> Sparsity : 0.0008396216228570436\n",
        "==============================\n",
        "<__main__.Loader object at 0x7fb5d94e2510> is ready to go\n",
        "activation_function: ELU(alpha=1.0)\n",
        "loading adjacency matrix\n",
        "successfully train loaded...\n",
        "don't split the matrix\n",
        "\n",
        "Train 1 ==============================\n",
        "gum_temp: 0.7\n",
        "EPOCH[1/4] loss0.386\n",
        "train time: 534.3337316513062\n",
        "decay gum_temp: 0.6965087354348776\n",
        "model save...\n",
        "Valid ==================================================\n",
        "valid mode\n",
        "{'precision': array([0.02332116, 0.01668453, 0.0136216 , 0.01180293, 0.01056637]), 'recall': array([0.07605411, 0.10674152, 0.13055237, 0.1500007 , 0.16690163]), 'ndcg': array([0.06059464, 0.07001512, 0.07663515, 0.08171554, 0.08589203])}\n",
        "Test ==================================================\n",
        "test mode\n",
        "{'precision': array([0.02286154, 0.01636747, 0.01323375, 0.01149692, 0.01028535]), 'recall': array([0.0733682 , 0.10507128, 0.12697428, 0.14644623, 0.16393784]), 'ndcg': array([0.05854139, 0.06809456, 0.07418164, 0.07923892, 0.08345424])}\n",
        "\n",
        "Train 2 ==============================\n",
        "gum_temp: 0.6965087354348776\n",
        "EPOCH[2/4] loss0.125\n",
        "train time: 533.7940173149109\n",
        "decay gum_temp: 0.6930348836244177\n",
        "model save...\n",
        "Valid ==================================================\n",
        "valid mode\n",
        "{'precision': array([0.02645946, 0.0188398 , 0.01532527, 0.01327243, 0.01181431]), 'recall': array([0.08573863, 0.11987728, 0.14631547, 0.1682178 , 0.18665247]), 'ndcg': array([0.06959012, 0.08001479, 0.0873653 , 0.09306554, 0.09759169])}\n",
        "Test ==================================================\n",
        "test mode\n",
        "{'precision': array([0.02595954, 0.01831335, 0.01496528, 0.01297558, 0.01155402]), 'recall': array([0.08334737, 0.11692721, 0.14290043, 0.16482907, 0.18364577]), 'ndcg': array([0.06762457, 0.07773272, 0.08496656, 0.09062759, 0.09517945])}\n",
        "\n",
        "Train 3 ==============================\n",
        "gum_temp: 0.6930348836244177\n",
        "EPOCH[3/4] loss0.090\n",
        "train time: 534.7814464569092\n",
        "decay gum_temp: 0.6895783577221438\n",
        "model save...\n",
        "Valid ==================================================\n",
        "valid mode\n",
        "{'precision': array([0.02750109, 0.01956158, 0.01601076, 0.01382172, 0.01241183]), 'recall': array([0.08927831, 0.12504565, 0.1532377 , 0.1750567 , 0.19631655]), 'ndcg': array([0.07269772, 0.08360102, 0.09145334, 0.09717201, 0.10236788])}\n",
        "Test ==================================================\n",
        "test mode\n",
        "{'precision': array([0.02699444, 0.01916404, 0.01566191, 0.01349973, 0.01205774]), 'recall': array([0.08641382, 0.12267856, 0.15009758, 0.17216471, 0.19164879]), 'ndcg': array([0.07056823, 0.08149902, 0.08911955, 0.09480704, 0.09956793])}\n",
        "\n",
        "Train 4 ==============================\n",
        "gum_temp: 0.6895783577221438\n",
        "EPOCH[4/4] loss0.077\n",
        "train time: 535.70445728302\n",
        "decay gum_temp: 0.6861390713147286\n",
        "model save...\n",
        "Valid ==================================================\n",
        "valid mode\n",
        "{'precision': array([0.02835181, 0.02017617, 0.01645622, 0.01423201, 0.01270456]), 'recall': array([0.09210259, 0.12944523, 0.1578907 , 0.1813253 , 0.20186935]), 'ndcg': array([0.07493502, 0.08629033, 0.09422578, 0.10031303, 0.10533204])}\n",
        "Test ==================================================\n",
        "test mode\n",
        "{'precision': array([0.02766093, 0.01966642, 0.01600688, 0.01386563, 0.01239936]), 'recall': array([0.08931023, 0.12607232, 0.15384466, 0.1774857 , 0.19812262]), 'ndcg': array([0.07260988, 0.08374147, 0.09144997, 0.09752696, 0.1025441 ])}\n",
        "```\n",
        "\n",
        "We saved the trained model, outputs, and other artifacts like Tensorboard metrics. Here is the folder structure and size of these artifacts:\n",
        "\n",
        "```jsx\n",
        ".\n",
        "‚îú‚îÄ‚îÄ [ 12K]  checkpoints\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ [8.0K]  HMLET_End\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ [4.0K]  gowalla\n",
        "‚îú‚îÄ‚îÄ [ 13M]  data\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ [ 13M]  gowalla\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ [6.9M]  s_pre_adj_mat_train.npz\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ [753K]  test.txt\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ [4.4M]  train.txt\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ [752K]  val.txt\n",
        "‚îú‚îÄ‚îÄ [ 15K]  excel\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ [5.4K]  test_HMLET_End_512_0.7_0.005_0.01_1_3_1_0.6_[10, 20, 30, 40, 50].xlsx\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ [5.4K]  valid_HMLET_End_512_0.7_0.005_0.01_1_3_1_0.6_[10, 20, 30, 40, 50].xlsx\n",
        "‚îî‚îÄ‚îÄ [218K]  tensorboard\n",
        "    ‚îú‚îÄ‚îÄ [ 30K]  11-18-09h39m04s-\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ [ 26K]  events.out.tfevents.1637228346.9f9d02c6c3c0.161.0\n",
        "    ‚îú‚îÄ‚îÄ [ 43K]  11-18-09h56m39s-\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ [ 39K]  events.out.tfevents.1637229401.9f9d02c6c3c0.432.0\n",
        "    ‚îî‚îÄ‚îÄ [141K]  11-18-10h21m54s-\n",
        "        ‚îú‚îÄ‚îÄ [ 77K]  events.out.tfevents.1637230914.9f9d02c6c3c0.432.1\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_10\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_20\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_30\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_40\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_50\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_10\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_20\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_30\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_40\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_50\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_10\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_20\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_30\n",
        "        ‚îú‚îÄ‚îÄ [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_40\n",
        "        ‚îî‚îÄ‚îÄ [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_50\n",
        "\n",
        "  13M used in 25 directories, 9 files\n",
        "```\n",
        "\n",
        "Moreover, we repeated the experimental analysis on 2 other datasets - Amazon books, and Yelp 2018 business dataset. The procedure is exactly same. Here are the Jupyter notebooks for these experiments.\n",
        "\n",
        "1. [Learning Graph Embeddings with HMLET model on Yelp dataset](https://nbviewer.org/gist/sparsh-ai/da0a1e723f8675ae59a7273bf78a49a6)\n",
        "2. [Learning Graph Embeddings with HMLET model on Gowalla dataset](https://nbviewer.org/gist/sparsh-ai/a46167213b2b308953b2654381c8725b)\n",
        "3. [Learning Graph Embeddings with HMLET model on Amazon-books dataset](https://nbviewer.org/gist/sparsh-ai/0ac3a7e2bb7c507a417ed6953a8b0ff1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu4ptobIZI0f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}