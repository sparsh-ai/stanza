{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T355636 | TAGNN Session-based Recommendations on Sample Session dataset",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb44ESxvJOL5X/eDsIwrrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sparsh-ai/stanza/blob/S969796/nbs/T355636_TAGNN_Session_based_Recommendations_on_Sample_Session_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4fNVWWSTgWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac4012d-673a-4feb-9e75-536da4259f85"
      },
      "source": [
        "!wget -q --show-progress -O data_preprocessing.ipynb https://gist.githubusercontent.com/sparsh-ai/12d1f5ca07add606f27b0f841b550a82/raw/21f822713c8e6d61bf444b7d25ae82a78f121e50/t182546-preprocessing-of-sample-data-for-session-based-recommendations.ipynb\n",
        "%run data_preprocessing.ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_preprocessing. 100%[===================>]  14.23K  --.-KB/s    in 0.001s  \n",
            "sample_train-item-v 100%[===================>] 386.09K  --.-KB/s    in 0.04s   \n",
            "session_id;user_id;item_id;timeframe;eventdate\n",
            "1;NA;81766;526309;2016-05-09\n",
            "1;NA;31331;1031018;2016-05-09\n",
            "1;NA;32118;243569;2016-05-09\n",
            "1;NA;9654;75848;2016-05-09\n",
            "1;NA;32627;1112408;2016-05-09\n",
            "1;NA;33043;173912;2016-05-09\n",
            "1;NA;12352;329870;2016-05-09\n",
            "1;NA;35077;390072;2016-05-09\n",
            "1;NA;36118;487369;2016-05-09\n",
            "-- Starting @ 2021-11-26 08:16:55.297464s\n",
            "-- Reading data @ 2021-11-26 08:16:55.436236s\n",
            "Splitting date 1464134400.0\n",
            "469\n",
            "47\n",
            "[('2671', 1451952000.0), ('1211', 1452384000.0), ('3780', 1452384000.0)]\n",
            "[('1864', 1464220800.0), ('1867', 1464220800.0), ('1868', 1464220800.0)]\n",
            "-- Splitting train set and test set @ 2021-11-26 08:16:55.448963s\n",
            "310\n",
            "1205\n",
            "99\n",
            "[[1, 2], [1], [4]] [1451952000.0, 1451952000.0, 1452384000.0] [3, 2, 5]\n",
            "[[282], [281, 308], [281]] [1464220800.0, 1464220800.0, 1464220800.0] [282, 281, 308]\n",
            "avg length:  3.5669291338582676\n",
            "Done.\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            ".\n",
            "├── [6.4K]  all_train_seq.txt\n",
            "├── [ 14K]  data_preprocessing.ipynb\n",
            "├── [ 54M]  sample_data\n",
            "│   ├── [1.7K]  anscombe.json\n",
            "│   ├── [294K]  california_housing_test.csv\n",
            "│   ├── [1.6M]  california_housing_train.csv\n",
            "│   ├── [ 17M]  mnist_test.csv\n",
            "│   ├── [ 35M]  mnist_train_small.csv\n",
            "│   └── [ 930]  README.md\n",
            "├── [386K]  sample_train-item-views.csv\n",
            "├── [1.2K]  test.txt\n",
            "└── [ 18K]  train.txt\n",
            "\n",
            "  55M used in 1 directory, 11 files\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 2.1.2 which is incompatible.\u001b[0m\n",
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2021-11-26 08:17:05\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.104+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "csv     : 1.0\n",
            "networkx: 2.6.3\n",
            "numpy   : 1.19.5\n",
            "torch   : 1.10.0+cu111\n",
            "IPython : 5.5.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTkBob7DTs2L"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import datetime\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk_7AdvECZxr"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH4N35xr90o9"
      },
      "source": [
        "def build_graph(train_data):\n",
        "    graph = nx.DiGraph()\n",
        "    for seq in train_data:\n",
        "        for i in range(len(seq) - 1):\n",
        "            if graph.get_edge_data(seq[i], seq[i + 1]) is None:\n",
        "                weight = 1\n",
        "            else:\n",
        "                weight = graph.get_edge_data(seq[i], seq[i + 1])['weight'] + 1\n",
        "            graph.add_edge(seq[i], seq[i + 1], weight=weight)\n",
        "    for node in graph.nodes:\n",
        "        sum = 0\n",
        "        for j, i in graph.in_edges(node):\n",
        "            sum += graph.get_edge_data(j, i)['weight']\n",
        "        if sum != 0:\n",
        "            for j, i in graph.in_edges(i):\n",
        "                graph.add_edge(j, i, weight=graph.get_edge_data(j, i)['weight'] / sum)\n",
        "    return graph\n",
        "\n",
        "\n",
        "def data_masks(all_usr_pois, item_tail):\n",
        "    us_lens = [len(upois) for upois in all_usr_pois]\n",
        "    len_max = max(us_lens)\n",
        "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
        "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
        "    return us_pois, us_msks, len_max\n",
        "\n",
        "\n",
        "def split_validation(train_set, valid_portion):\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
        "\n",
        "\n",
        "class Data():\n",
        "    def __init__(self, data, shuffle=False, graph=None):\n",
        "        inputs = data[0]\n",
        "        inputs, mask, len_max = data_masks(inputs, [0])\n",
        "        self.inputs = np.asarray(inputs)\n",
        "        self.mask = np.asarray(mask)\n",
        "        self.len_max = len_max\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(inputs)\n",
        "        self.shuffle = shuffle\n",
        "        self.graph = graph\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.inputs = self.inputs[shuffled_arg]\n",
        "            self.mask = self.mask[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, i):\n",
        "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
        "        items, n_node, A, alias_inputs = [], [], [], []\n",
        "        for u_input in inputs:\n",
        "            n_node.append(len(np.unique(u_input)))\n",
        "        max_n_node = np.max(n_node)\n",
        "        for u_input in inputs:\n",
        "            node = np.unique(u_input)\n",
        "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "            u_A = np.zeros((max_n_node, max_n_node))\n",
        "            for i in np.arange(len(u_input) - 1):\n",
        "                if u_input[i + 1] == 0:\n",
        "                    break\n",
        "                u = np.where(node == u_input[i])[0][0]\n",
        "                v = np.where(node == u_input[i + 1])[0][0]\n",
        "                u_A[u][v] = 1\n",
        "            u_sum_in = np.sum(u_A, 0)\n",
        "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "            u_A_in = np.divide(u_A, u_sum_in)\n",
        "            u_sum_out = np.sum(u_A, 1)\n",
        "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
        "            A.append(u_A)\n",
        "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "        return alias_inputs, A, items, mask, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQaHam5ALl_E"
      },
      "source": [
        "class GNN(Module):\n",
        "    def __init__(self, hidden_size, step=1):\n",
        "        super(GNN, self).__init__()\n",
        "        self.step = step\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = hidden_size * 2\n",
        "        self.gate_size = 3 * hidden_size\n",
        "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
        "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
        "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
        "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
        "\n",
        "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "\n",
        "    def GNNCell(self, A, hidden):\n",
        "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
        "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n",
        "        inputs = torch.cat([input_in, input_out], 2)\n",
        "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
        "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
        "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
        "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
        "        resetgate = torch.sigmoid(i_r + h_r)\n",
        "        inputgate = torch.sigmoid(i_i + h_i)\n",
        "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
        "        hy = newgate + inputgate * (hidden - newgate)\n",
        "        return hy\n",
        "\n",
        "    def forward(self, A, hidden):\n",
        "        for i in range(self.step):\n",
        "            hidden = self.GNNCell(A, hidden)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class SessionGraph(Module):\n",
        "    def __init__(self, opt, n_node):\n",
        "        super(SessionGraph, self).__init__()\n",
        "        self.hidden_size = opt.hiddenSize\n",
        "        self.n_node = n_node\n",
        "        self.batch_size = opt.batchSize\n",
        "        self.nonhybrid = opt.nonhybrid\n",
        "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
        "        self.gnn = GNN(self.hidden_size, step=opt.step)\n",
        "        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n",
        "        self.linear_t = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  #target attention\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def compute_scores(self, hidden, mask):\n",
        "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
        "        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size\n",
        "        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size\n",
        "        alpha = self.linear_three(torch.sigmoid(q1 + q2))  # (b,s,1)\n",
        "        # alpha = torch.sigmoid(alpha) # B,S,1\n",
        "        alpha = F.softmax(alpha, 1) # B,S,1\n",
        "        a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)  # (b,d)\n",
        "        if not self.nonhybrid:\n",
        "            a = self.linear_transform(torch.cat([a, ht], 1))\n",
        "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
        "        # target attention: sigmoid(hidden M b)\n",
        "        # mask  # batch_size x seq_length\n",
        "        hidden = hidden * mask.view(mask.shape[0], -1, 1).float()  # batch_size x seq_length x latent_size\n",
        "        qt = self.linear_t(hidden)  # batch_size x seq_length x latent_size\n",
        "        # beta = torch.sigmoid(b @ qt.transpose(1,2))  # batch_size x n_nodes x seq_length\n",
        "        beta = F.softmax(b @ qt.transpose(1,2), -1)  # batch_size x n_nodes x seq_length\n",
        "        target = beta @ hidden  # batch_size x n_nodes x latent_size\n",
        "        a = a.view(ht.shape[0], 1, ht.shape[1])  # b,1,d\n",
        "        a = a + target  # b,n,d\n",
        "        scores = torch.sum(a * b, -1)  # b,n\n",
        "        # scores = torch.matmul(a, b.transpose(1, 0))\n",
        "        return scores\n",
        "\n",
        "    def forward(self, inputs, A):\n",
        "        hidden = self.embedding(inputs)\n",
        "        hidden = self.gnn(A, hidden)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "def trans_to_cuda(variable):\n",
        "    if torch.cuda.is_available():\n",
        "        return variable.cuda()\n",
        "    else:\n",
        "        return variable\n",
        "\n",
        "\n",
        "def trans_to_cpu(variable):\n",
        "    if torch.cuda.is_available():\n",
        "        return variable.cpu()\n",
        "    else:\n",
        "        return variable\n",
        "\n",
        "\n",
        "def forward(model, i, data):\n",
        "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
        "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
        "    items = trans_to_cuda(torch.Tensor(items).long())\n",
        "    A = trans_to_cuda(torch.Tensor(A).float())\n",
        "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
        "    hidden = model(items, A)\n",
        "    get = lambda i: hidden[i][alias_inputs[i]]\n",
        "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
        "    return targets, model.compute_scores(seq_hidden, mask)\n",
        "\n",
        "\n",
        "def train_test(model, train_data, test_data):\n",
        "    model.scheduler.step()\n",
        "    print('start training: ', datetime.datetime.now())\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    slices = train_data.generate_batch(model.batch_size)\n",
        "    for i, j in zip(slices, np.arange(len(slices))):\n",
        "        model.optimizer.zero_grad()\n",
        "        targets, scores = forward(model, i, train_data)\n",
        "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
        "        loss = model.loss_function(scores, targets - 1)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if j % int(len(slices) / 5 + 1) == 0:\n",
        "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
        "    print('\\tLoss:\\t%.3f' % total_loss)\n",
        "\n",
        "    print('start predicting: ', datetime.datetime.now())\n",
        "    model.eval()\n",
        "    hit, mrr = [], []\n",
        "    slices = test_data.generate_batch(model.batch_size)\n",
        "    for i in slices:\n",
        "        targets, scores = forward(model, i, test_data)\n",
        "        sub_scores = scores.topk(20)[1]\n",
        "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
        "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
        "            hit.append(np.isin(target - 1, score))\n",
        "            if len(np.where(score == target - 1)[0]) == 0:\n",
        "                mrr.append(0)\n",
        "            else:\n",
        "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
        "    hit = np.mean(hit) * 100\n",
        "    mrr = np.mean(mrr) * 100\n",
        "    return hit, mrr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO5yEePq9ZtY"
      },
      "source": [
        "class Args():\n",
        "    dataset = 'sample'\n",
        "    batchSize = 100 # input batch size\n",
        "    hiddenSize = 100 # hidden state size\n",
        "    epoch = 30 # the number of epochs to train for\n",
        "    lr = 0.001 # learning rate')  # [0.001, 0.0005, 0.000\n",
        "    lr_dc = 0.1 # learning rate decay rate\n",
        "    lr_dc_step = 3 # the number of steps after which the learning rate decay\n",
        "    l2 = 1e-5 # l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.0000\n",
        "    step = 1 # gnn propogation steps\n",
        "    patience = 10 # the number of epoch to wait before early stop \n",
        "    nonhybrid = True # only use the global preference to predict\n",
        "    validation = True # validation\n",
        "    valid_portion = 0.1 # split the portion of training set as validation set\n",
        "\n",
        "args = Args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCfErpBd9ZrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831191ed-2e56-499f-9042-59e96ccd9098"
      },
      "source": [
        "train_data = pickle.load(open('train.txt', 'rb'))\n",
        "\n",
        "if args.validation:\n",
        "    train_data, valid_data = split_validation(train_data, args.valid_portion)\n",
        "    test_data = valid_data\n",
        "else:\n",
        "    test_data = pickle.load(open('test.txt', 'rb'))\n",
        "\n",
        "train_data = Data(train_data, shuffle=True)\n",
        "test_data = Data(test_data, shuffle=False)\n",
        "\n",
        "n_node = 310\n",
        "\n",
        "model = trans_to_cuda(SessionGraph(args, n_node))\n",
        "\n",
        "start = time.time()\n",
        "best_result = [0, 0]\n",
        "best_epoch = [0, 0]\n",
        "bad_counter = 0\n",
        "\n",
        "for epoch in range(args.epoch):\n",
        "    print('-------------------------------------------------------')\n",
        "    print('epoch: ', epoch)\n",
        "    hit, mrr = train_test(model, train_data, test_data)\n",
        "    flag = 0\n",
        "    if hit >= best_result[0]:\n",
        "        best_result[0] = hit\n",
        "        best_epoch[0] = epoch\n",
        "        flag = 1\n",
        "    if mrr >= best_result[1]:\n",
        "        best_result[1] = mrr\n",
        "        best_epoch[1] = epoch\n",
        "        flag = 1\n",
        "    print('Best Result:')\n",
        "    print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
        "    bad_counter += 1 - flag\n",
        "    if bad_counter >= args.patience:\n",
        "        break\n",
        "print('-------------------------------------------------------')\n",
        "end = time.time()\n",
        "print(\"Run time: %f s\" % (end - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-11-26 08:18:19.342365\n",
            "[0/11] Loss: 5.7060\n",
            "[3/11] Loss: 5.7097\n",
            "[6/11] Loss: 5.7073\n",
            "[9/11] Loss: 5.7052\n",
            "\tLoss:\t62.742\n",
            "start predicting:  2021-11-26 08:18:20.009675\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-11-26 08:18:20.060626\n",
            "[0/11] Loss: 5.6878\n",
            "[3/11] Loss: 5.6802\n",
            "[6/11] Loss: 5.6788\n",
            "[9/11] Loss: 5.6788\n",
            "\tLoss:\t62.449\n",
            "start predicting:  2021-11-26 08:18:20.673947\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  2\n",
            "start training:  2021-11-26 08:18:20.718606\n",
            "[0/11] Loss: 5.6631\n",
            "[3/11] Loss: 5.6407\n",
            "[6/11] Loss: 5.6577\n",
            "[9/11] Loss: 5.6534\n",
            "\tLoss:\t62.175\n",
            "start predicting:  2021-11-26 08:18:21.333300\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  3\n",
            "start training:  2021-11-26 08:18:21.373369\n",
            "[0/11] Loss: 5.6554\n",
            "[3/11] Loss: 5.6460\n",
            "[6/11] Loss: 5.6411\n",
            "[9/11] Loss: 5.6209\n",
            "\tLoss:\t62.116\n",
            "start predicting:  2021-11-26 08:18:21.970481\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  4\n",
            "start training:  2021-11-26 08:18:22.008774\n",
            "[0/11] Loss: 5.6509\n",
            "[3/11] Loss: 5.6492\n",
            "[6/11] Loss: 5.6392\n",
            "[9/11] Loss: 5.6166\n",
            "\tLoss:\t62.045\n",
            "start predicting:  2021-11-26 08:18:22.667926\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  5\n",
            "start training:  2021-11-26 08:18:22.712264\n",
            "[0/11] Loss: 5.6191\n",
            "[3/11] Loss: 5.6374\n",
            "[6/11] Loss: 5.6412\n",
            "[9/11] Loss: 5.6584\n",
            "\tLoss:\t62.003\n",
            "start predicting:  2021-11-26 08:18:23.344109\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  6\n",
            "start training:  2021-11-26 08:18:23.383819\n",
            "[0/11] Loss: 5.6572\n",
            "[3/11] Loss: 5.6284\n",
            "[6/11] Loss: 5.6442\n",
            "[9/11] Loss: 5.6176\n",
            "\tLoss:\t61.997\n",
            "start predicting:  2021-11-26 08:18:23.979931\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  7\n",
            "start training:  2021-11-26 08:18:24.021037\n",
            "[0/11] Loss: 5.6503\n",
            "[3/11] Loss: 5.6199\n",
            "[6/11] Loss: 5.6507\n",
            "[9/11] Loss: 5.6389\n",
            "\tLoss:\t61.987\n",
            "start predicting:  2021-11-26 08:18:24.628050\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  8\n",
            "start training:  2021-11-26 08:18:24.668411\n",
            "[0/11] Loss: 5.6125\n",
            "[3/11] Loss: 5.6451\n",
            "[6/11] Loss: 5.6416\n",
            "[9/11] Loss: 5.6288\n",
            "\tLoss:\t61.981\n",
            "start predicting:  2021-11-26 08:18:25.293683\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  9\n",
            "start training:  2021-11-26 08:18:25.338027\n",
            "[0/11] Loss: 5.6348\n",
            "[3/11] Loss: 5.6128\n",
            "[6/11] Loss: 5.6488\n",
            "[9/11] Loss: 5.6403\n",
            "\tLoss:\t61.980\n",
            "start predicting:  2021-11-26 08:18:25.954176\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  10\n",
            "start training:  2021-11-26 08:18:25.997571\n",
            "[0/11] Loss: 5.6278\n",
            "[3/11] Loss: 5.6519\n",
            "[6/11] Loss: 5.6326\n",
            "[9/11] Loss: 5.6364\n",
            "\tLoss:\t61.976\n",
            "start predicting:  2021-11-26 08:18:26.626378\n",
            "Best Result:\n",
            "\tRecall@20:\t61.1570\tMMR@20:\t43.4304\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "Run time: 7.327047 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuDoPVTyU6Ku"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3MdwS_RU6Kv"
      },
      "source": [
        "!apt-get -qq install tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlRTFfEXU6Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6275b7d3-bf3e-410a-d993-319f9c960412"
      },
      "source": [
        "!tree -h --du ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "├── [6.4K]  all_train_seq.txt\n",
            "├── [ 14K]  data_preprocessing.ipynb\n",
            "├── [ 54M]  sample_data\n",
            "│   ├── [1.7K]  anscombe.json\n",
            "│   ├── [294K]  california_housing_test.csv\n",
            "│   ├── [1.6M]  california_housing_train.csv\n",
            "│   ├── [ 17M]  mnist_test.csv\n",
            "│   ├── [ 35M]  mnist_train_small.csv\n",
            "│   └── [ 930]  README.md\n",
            "├── [386K]  sample_train-item-views.csv\n",
            "├── [1.2K]  test.txt\n",
            "└── [ 18K]  train.txt\n",
            "\n",
            "  55M used in 1 directory, 11 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0es6P7U6Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d0a33d-a45e-4846-f326-799e2ea05ff1"
      },
      "source": [
        "!pip install -q watermark\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2021-11-26 08:19:39\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.104+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "csv     : 1.0\n",
            "networkx: 2.6.3\n",
            "numpy   : 1.19.5\n",
            "torch   : 1.10.0+cu111\n",
            "IPython : 5.5.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpW4NFdUU6Kw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izsndoKwU6Kx"
      },
      "source": [
        "**END**"
      ]
    }
  ]
}